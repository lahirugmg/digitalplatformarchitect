# Feature: Code Generation & Export

**Priority:** High ðŸŸ¢  
**Category:** Developer Productivity  
**Effort:** Large (5-6 weeks)  
**Impact:** Very High

## Overview
Extend the current IaC export functionality to generate complete application code, configuration files, CI/CD pipelines, and deployment scripts. Transform visual pipeline designs into production-ready code across multiple languages and frameworks.

## Current State
- IaC export exists ([lib/iac-export.ts](lib/iac-export.ts))
- Supports Terraform and CloudFormation
- AWS, Azure, GCP providers
- Limited to infrastructure code only
- No application logic, no deployment automation

## Problem Statement
Users design pipelines visually but still need to:
- Write application code manually
- Configure services manually
- Set up CI/CD manually
- Create deployment scripts manually
- Wire everything together manually

This creates a gap between design and implementation.

## Proposed Solution

### Expanded Export Capabilities

#### 1. Enhanced Export Modal
```tsx
// components/ExportModal.tsx
<ExportModal>
  <Tabs>
    <Tab name="Infrastructure">
      <InfrastructureExport />
    </Tab>
    <Tab name="Application Code">
      <ApplicationCodeExport />
    </Tab>
    <Tab name="CI/CD Pipeline">
      <CICDExport />
    </Tab>
    <Tab name="Deployment">
      <DeploymentExport />
    </Tab>
    <Tab name="Documentation">
      <DocumentationExport />
    </Tab>
  </Tabs>
</ExportModal>
```

#### 2. Infrastructure as Code (Enhanced)
Current + new providers:
```tsx
<InfrastructureOptions>
  <Select label="Format">
    <option>Terraform (HCL)</option>
    <option>CloudFormation (YAML)</option>
    <option>Pulumi (TypeScript)</option>
    <option>CDK (TypeScript)</option>
    <option>Bicep (Azure)</option>
    <option>ARM Templates</option>
  </Select>
  
  <Select label="Provider">
    <option>AWS</option>
    <option>Azure</option>
    <option>GCP</option>
    <option>Kubernetes</option>
    <option>Multi-Cloud</option>
  </Select>
  
  <Checkbox>Include networking</Checkbox>
  <Checkbox>Include IAM policies</Checkbox>
  <Checkbox>Include monitoring</Checkbox>
  <Checkbox>Include backup configuration</Checkbox>
</InfrastructureOptions>
```

#### 3. Application Code Generation
```tsx
<ApplicationCodeOptions>
  <Select label="Language">
    <option>Python</option>
    <option>Node.js (TypeScript)</option>
    <option>Java</option>
    <option>Go</option>
    <option>C#</option>
  </Select>
  
  <Select label="Framework">
    {/* Dynamic based on language */}
    <option>FastAPI (Python)</option>
    <option>Flask (Python)</option>
    <option>Express (Node)</option>
    <option>Nest.js (Node)</option>
    <option>Spring Boot (Java)</option>
  </Select>
  
  <Checkbox>Include unit tests</Checkbox>
  <Checkbox>Include integration tests</Checkbox>
  <Checkbox>Include Docker files</Checkbox>
  <Checkbox>Include API documentation</Checkbox>
</ApplicationCodeOptions>
```

Example generated Lambda function:
```python
# functions/data_processor/handler.py
import json
import boto3
from typing import Dict, Any

# Initialize AWS clients
s3 = boto3.client('s3')
dynamodb = boto3.resource('dynamodb')
table = dynamodb.Table('ProcessedData')

def lambda_handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:
    """
    Process incoming data from Kinesis stream.
    Transform and store in DynamoDB.
    
    Generated by Digital Platform Architect
    """
    try:
        # Parse Kinesis records
        for record in event['Records']:
            # Decode data
            payload = json.loads(record['kinesis']['data'])
            
            # Transform data
            processed = transform_data(payload)
            
            # Store in DynamoDB
            table.put_item(Item=processed)
        
        return {
            'statusCode': 200,
            'body': json.dumps('Success')
        }
    
    except Exception as e:
        print(f"Error: {str(e)}")
        return {
            'statusCode': 500,
            'body': json.dumps(f'Error: {str(e)}')
        }

def transform_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """Apply business logic transformations."""
    return {
        'id': data['id'],
        'timestamp': data['timestamp'],
        'value': data['value'] * 1.1,  # Example transformation
        'processed_at': datetime.now().isoformat()
    }

# Unit tests included
def test_transform_data():
    input_data = {'id': '123', 'timestamp': '2024-01-01', 'value': 100}
    result = transform_data(input_data)
    assert result['value'] == 110
```

#### 4. CI/CD Pipeline Generation
```tsx
<CICDOptions>
  <Select label="Platform">
    <option>GitHub Actions</option>
    <option>GitLab CI</option>
    <option>Jenkins</option>
    <option>CircleCI</option>
    <option>AWS CodePipeline</option>
    <option>Azure DevOps</option>
  </Select>
  
  <Checkbox>Include linting</Checkbox>
  <Checkbox>Include security scanning</Checkbox>
  <Checkbox>Include automated tests</Checkbox>
  <Checkbox>Include deployment stages</Checkbox>
  <Checkbox>Include rollback capability</Checkbox>
</CICDOptions>
```

Example GitHub Actions workflow:
```yaml
# .github/workflows/deploy.yml
name: Deploy Data Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  AWS_REGION: us-east-1

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov
      
      - name: Run tests
        run: pytest tests/ --cov=src/
      
      - name: Lint code
        run: |
          pip install flake8
          flake8 src/ --max-line-length=120
  
  security:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Run security scan
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
  
  deploy-dev:
    needs: [test, security]
    runs-on: ubuntu-latest
    environment: development
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Deploy infrastructure
        run: |
          cd terraform/
          terraform init
          terraform plan -out=tfplan
          terraform apply tfplan
      
      - name: Deploy Lambda functions
        run: |
          cd functions/
          ./deploy.sh dev
  
  deploy-prod:
    needs: [deploy-dev]
    runs-on: ubuntu-latest
    environment: production
    if: github.ref == 'refs/heads/main'
    steps:
      # Similar to deploy-dev but for production
      - name: Deploy to production
        run: echo "Deploying to production..."
```

#### 5. Deployment Scripts
```tsx
<DeploymentOptions>
  <Select label="Deployment Tool">
    <option>Shell Scripts</option>
    <option>Ansible</option>
    <option>Helm Charts (K8s)</option>
    <option>Docker Compose</option>
    <option>Pulumi</option>
  </Select>
  
  <Select label="Environment">
    <option>Development</option>
    <option>Staging</option>
    <option>Production</option>
    <option>All (Multi-env)</option>
  </Select>
</DeploymentOptions>
```

Example deployment script:
```bash
#!/bin/bash
# deploy.sh - Generated deployment script

set -e

ENVIRONMENT=${1:-dev}
REGION=${2:-us-east-1}

echo "ðŸš€ Deploying data pipeline to $ENVIRONMENT..."

# Deploy infrastructure
echo "ðŸ“¦ Deploying infrastructure..."
cd terraform/
terraform init
terraform workspace select $ENVIRONMENT || terraform workspace new $ENVIRONMENT
terraform apply -auto-approve \
  -var="environment=$ENVIRONMENT" \
  -var="region=$REGION"

# Build and deploy Lambda functions
echo "âš¡ Building Lambda functions..."
cd ../functions/
for func in */; do
  echo "Building $func..."
  cd $func
  pip install -r requirements.txt -t .
  zip -r ../deploy/$func.zip .
  cd ..
done

# Deploy functions
echo "ðŸ“¤ Deploying functions..."
aws lambda update-function-code \
  --function-name data-processor-$ENVIRONMENT \
  --zip-file fileb://deploy/data_processor.zip

# Run smoke tests
echo "ðŸ§ª Running smoke tests..."
npm run test:smoke

echo "âœ… Deployment complete!"
```

#### 6. Documentation Generation
```tsx
<DocumentationOptions>
  <Select label="Format">
    <option>Markdown</option>
    <option>HTML (Static Site)</option>
    <option>PDF</option>
    <option>Confluence</option>
    <option>Notion</option>
  </Select>
  
  <Checkbox>Include architecture diagram</Checkbox>
  <Checkbox>Include API documentation</Checkbox>
  <Checkbox>Include deployment guide</Checkbox>
  <Checkbox>Include troubleshooting</Checkbox>
  <Checkbox>Include cost breakdown</Checkbox>
</DocumentationOptions>
```

Generated README.md:
```markdown
# Data Pipeline Architecture

Generated by Digital Platform Architect - [Design Link](...)

## Overview

This data pipeline processes IoT sensor data in real-time, performing transformations and storing results for analytics.

## Architecture

![Architecture Diagram](./docs/architecture.png)

### Components

- **IoT Sensors** â†’ AWS IoT Core
- **Stream Processing** â†’ Amazon Kinesis
- **Data Transformation** â†’ AWS Lambda
- **Storage** â†’ Amazon DynamoDB
- **Analytics** â†’ Amazon Athena

## Prerequisites

- AWS Account with appropriate permissions
- Terraform >= 1.0
- Python >= 3.11
- AWS CLI configured

## Deployment

### Quick Start
\`\`\`bash
./deploy.sh dev
\`\`\`

### Manual Deployment
\`\`\`bash
# 1. Deploy infrastructure
cd terraform/
terraform init
terraform apply

# 2. Deploy application code
cd ../functions/
./build.sh
./deploy.sh
\`\`\`

## Configuration

See `config/` directory for environment-specific settings.

## Monitoring

- CloudWatch Dashboard: [Link](...)
- Logs: CloudWatch Log Groups
- Metrics: Custom metrics in CloudWatch

## Cost Estimate

Estimated monthly cost: **$342.50**

See [cost breakdown](./docs/cost-analysis.md) for details.

## Troubleshooting

Common issues and solutions...
```

## Code Generation Architecture

### Template Engine
```typescript
// lib/code-generator.ts
import Handlebars from 'handlebars'

interface CodeTemplate {
  language: string
  framework: string
  template: string
  dependencies: string[]
}

class CodeGenerator {
  private templates: Map<string, CodeTemplate>
  
  constructor() {
    this.loadTemplates()
  }
  
  generate(pipeline: Pipeline, options: GenerationOptions): GeneratedCode {
    const template = this.getTemplate(options.language, options.framework)
    const context = this.buildContext(pipeline)
    const code = Handlebars.compile(template.template)(context)
    
    return {
      files: this.organizeFiles(code),
      dependencies: template.dependencies,
      instructions: this.generateInstructions(options)
    }
  }
  
  private buildContext(pipeline: Pipeline) {
    return {
      services: pipeline.nodes.map(this.nodeToService),
      connections: pipeline.edges.map(this.edgeToConnection),
      config: this.extractConfig(pipeline),
      metadata: this.generateMetadata()
    }
  }
}
```

### Service Mapping
```typescript
// Map visual nodes to code constructs
const serviceMappings = {
  'aws-lambda': {
    python: {
      template: 'templates/python/lambda.py.hbs',
      tests: 'templates/python/test_lambda.py.hbs',
      requirements: ['boto3', 'pytest']
    },
    nodejs: {
      template: 'templates/nodejs/lambda.ts.hbs',
      tests: 'templates/nodejs/lambda.test.ts.hbs',
      dependencies: ['@aws-sdk/client-lambda', 'jest']
    }
  },
  'aws-kinesis': {
    python: {
      template: 'templates/python/kinesis_producer.py.hbs',
      requirements: ['boto3', 'aws-kinesis-agg']
    }
  }
}
```

## Export Formats

### 1. Single File
Download single file (e.g., `pipeline.tf`)

### 2. ZIP Archive
Download complete project as ZIP:
```
pipeline-export.zip/
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ outputs.tf
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ data_processor/
â”‚   â”‚   â”œâ”€â”€ handler.py
â”‚   â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â””â”€â”€ data_transformer/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ architecture.png
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy.sh
â”‚   â””â”€â”€ rollback.sh
â””â”€â”€ docker-compose.yml
```

### 3. Git Repository
Push directly to GitHub/GitLab:
```tsx
<GitExport>
  <input placeholder="Repository URL" />
  <input placeholder="Branch name" />
  <Checkbox>Create pull request</Checkbox>
  <Checkbox>Trigger CI/CD</Checkbox>
  <button>Push to Git</button>
</GitExport>
```

### 4. Project Scaffold
Interactive CLI to setup project:
```bash
npx create-dpa-project my-pipeline \
  --template=aws-kinesis-lambda \
  --language=python \
  --cicd=github-actions
```

## Advanced Features

### 1. Incremental Updates
```typescript
// Track changes and generate diffs
interface CodeDiff {
  added: string[]
  modified: string[]
  removed: string[]
  migrationSteps: string[]
}

// Only regenerate changed parts
const updateCode = (previousExport: Export, currentPipeline: Pipeline) => {
  const diff = computeDiff(previousExport, currentPipeline)
  return generateIncremental(diff)
}
```

### 2. Custom Templates
Allow users to create custom templates:
```tsx
<TemplateManager>
  <button onClick={createTemplate}>
    âž• Create Custom Template
  </button>
  
  <TemplateList>
    {customTemplates.map(template => (
      <TemplateCard
        name={template.name}
        description={template.description}
        language={template.language}
        onEdit={() => editTemplate(template)}
        onDelete={() => deleteTemplate(template)}
      />
    ))}
  </TemplateList>
</TemplateManager>
```

### 3. AI-Enhanced Generation
```typescript
// Use AI to generate better code
const enhanceWithAI = async (code: string) => {
  const improved = await openai.createCompletion({
    prompt: `Improve this code for production readiness:\n${code}`,
    model: 'gpt-4'
  })
  
  return improved.choices[0].text
}
```

### 4. Code Quality Tools
Integrate linting and formatting:
```typescript
const formatCode = async (code: string, language: string) => {
  switch (language) {
    case 'python':
      return await runBlack(code)
    case 'typescript':
      return await runPrettier(code)
    case 'go':
      return await runGofmt(code)
  }
}
```

## Implementation Phases

### Phase 1: Enhanced IaC (Week 1-2)
- Add Pulumi, CDK support
- Multi-cloud templates
- Better documentation generation

### Phase 2: Application Code (Week 3-4)
- Python code generation
- Node.js code generation
- Include tests and Docker files

### Phase 3: CI/CD & Deployment (Week 5)
- GitHub Actions workflows
- Deployment scripts
- Multi-environment support

### Phase 4: Advanced Features (Week 6)
- Custom templates
- Incremental updates
- Git integration
- Quality tools integration

## Success Metrics

- 80%+ of users export their designs
- 50%+ deploy generated code without modifications
- Generated code passes CI/CD checks
- Time from design to deployment reduced by 70%
- User satisfaction: 8+/10

## Dependencies

- Handlebars (templating)
- js-yaml (YAML generation)
- Prettier (code formatting)
- Octokit (GitHub API)
- jszip (ZIP file creation)

## Future Enhancements

1. **IDE Integration** - VS Code extension
2. **Live Preview** - See code changes in real-time
3. **Code Assistant** - AI-powered code suggestions
4. **Template Marketplace** - Share custom templates
5. **Migration Tools** - Import existing infrastructure
6. **Multi-Language Support** - More languages
7. **Framework Plugins** - Custom framework support

## References

- [Pulumi](https://www.pulumi.com/)
- [AWS CDK](https://aws.amazon.com/cdk/)
- [Terraform](https://www.terraform.io/)
- [GitHub Actions](https://github.com/features/actions)
